{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define data\n",
    "train_data = ['very well done',\n",
    "        'good work done by the team',\n",
    "        'you took great effort',\n",
    "        'nice work done by the members',\n",
    "        'excellent efforts on part of everyone',\n",
    "        'poor effort',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'could have done better']\n",
    "# define sentiment labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'better',\n",
       " 'by',\n",
       " 'could',\n",
       " 'done',\n",
       " 'effort',\n",
       " 'efforts',\n",
       " 'everyone',\n",
       " 'excellent',\n",
       " 'good',\n",
       " 'great',\n",
       " 'have',\n",
       " 'members',\n",
       " 'nice',\n",
       " 'not',\n",
       " 'of',\n",
       " 'on',\n",
       " 'part',\n",
       " 'poor',\n",
       " 'team',\n",
       " 'the',\n",
       " 'took',\n",
       " 'very',\n",
       " 'well',\n",
       " 'work',\n",
       " 'you'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase all words and create a vocabulary\n",
    "all_eng_words=set()\n",
    "for eng in train_data:\n",
    "    for word in eng.split():\n",
    "            all_eng_words.add(word.lower())\n",
    "all_eng_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_encoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_tokens += 1 #for zero padding\n",
    "num_encoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "reverse_input_index = dict((i, word) for word, i in input_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The embedding layer takes at least 2 arguments\n",
    "#### no of words in vocab + 1 (for padding)\n",
    "#### and the dimensionality of the embedding\n",
    "#### This layer can only be used as the first layer in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'excellent efforts on part of everyone'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_src=max(train_data, key=len)\n",
    "max_length_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_src=len(max_length_src.split())\n",
    "max_length_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 6, 5)              130       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 161\n",
      "Trainable params: 161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#emb_dim = 5\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_encoder_tokens, \n",
    "                    5, \n",
    "                    input_length=max_length_src))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters of embedding layer = token_size*no of word embeddiing dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros((9, max_length_src),dtype='float32')\n",
    "for i, input_text in enumerate(train_data):\n",
    "         for t, word in enumerate(input_text.split()):\n",
    "                # encoder input seq\n",
    "                encoder_input_data[i, t] = input_token_index[word.lower()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22., 23.,  4.,  0.,  0.,  0.],\n",
       "       [ 9., 24.,  4.,  2., 20., 19.],\n",
       "       [25., 21., 10.,  5.,  0.,  0.],\n",
       "       [13., 24.,  4.,  2., 20., 12.],\n",
       "       [ 8.,  6., 16., 17., 15.,  7.],\n",
       "       [18.,  5.,  0.,  0.,  0.,  0.],\n",
       "       [14.,  9.,  0.,  0.,  0.,  0.],\n",
       "       [18., 24.,  0.,  0.,  0.,  0.],\n",
       "       [ 3., 11.,  4.,  1.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6957 - accuracy: 0.4444\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6934 - accuracy: 0.5556\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6911 - accuracy: 0.5556\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6888 - accuracy: 0.5556\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6865 - accuracy: 0.6667\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6842 - accuracy: 0.7778\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6819 - accuracy: 0.7778\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6796 - accuracy: 0.7778\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6773 - accuracy: 0.7778\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6750 - accuracy: 0.7778\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6727 - accuracy: 0.7778\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6705 - accuracy: 0.8889\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6682 - accuracy: 0.8889\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6659 - accuracy: 0.8889\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6636 - accuracy: 0.8889\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6613 - accuracy: 0.8889\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6591 - accuracy: 1.00 - 0s 4ms/step - loss: 0.6591 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6568 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6545 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6522 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6499 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6475 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6452 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6406 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6382 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6359 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6335 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6311 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6287 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6263 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6239 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6215 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6190 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6166 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6141 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6116 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6092 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6067 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6041 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6016 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5991 - accuracy: 1.00 - 0s 6ms/step - loss: 0.5991 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5965 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5939 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5914 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5888 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 1.00 - 0s 15ms/step - loss: 0.5862 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5835 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5809 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5783 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26823a5b668>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(encoder_input_data, labels, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47042567]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data ='not done well'\n",
    "test_data = np.array([[14., 4.,23., 0. ,0. ,0.]]).reshape(1,6)\n",
    "model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01833678, -0.04171934,  0.09368385,  0.03802868,  0.00325983],\n",
       "       [ 0.02218266, -0.0977444 ,  0.0927205 , -0.0005423 ,  0.09035153],\n",
       "       [-0.0228668 ,  0.00767461, -0.07337783, -0.02228965, -0.06538216],\n",
       "       [ 0.00376928, -0.00826061,  0.06869004, -0.07292867, -0.0898812 ],\n",
       "       [-0.04430699, -0.0801262 , -0.02910785, -0.071634  ,  0.06116445],\n",
       "       [-0.08186093,  0.03130139, -0.08237974, -0.01509692,  0.08851887],\n",
       "       [ 0.03175548, -0.06967256,  0.02768652, -0.0198629 , -0.00728823],\n",
       "       [ 0.08193626,  0.0951513 , -0.05666602, -0.06466027,  0.06475899],\n",
       "       [ 0.01560468,  0.04749871, -0.01863059,  0.01792593,  0.00734033],\n",
       "       [-0.0885355 ,  0.07014767, -0.06155946,  0.06812058,  0.01431042],\n",
       "       [-0.0586049 , -0.00215694, -0.05603223, -0.06199695,  0.046665  ],\n",
       "       [-0.02168677,  0.04412618, -0.04789738,  0.05795659,  0.07937761],\n",
       "       [ 0.05145985,  0.06986363, -0.08783805, -0.03640277,  0.03809496],\n",
       "       [-0.01188653,  0.08303266, -0.07251692,  0.09795833,  0.00846821],\n",
       "       [-0.05336507, -0.09700217,  0.0149353 , -0.02101807, -0.06435829],\n",
       "       [ 0.05837001,  0.07757801,  0.04775926, -0.00354668, -0.05105806],\n",
       "       [-0.04781444, -0.06560271, -0.02251158, -0.01685836,  0.00501009],\n",
       "       [-0.08064097,  0.06866971, -0.08719493, -0.01714157, -0.00985219],\n",
       "       [-0.02206943, -0.04260797,  0.04215139, -0.09841257, -0.02165589],\n",
       "       [ 0.02598086,  0.00130202, -0.04592861, -0.0222191 ,  0.09112859],\n",
       "       [ 0.01785637,  0.04975023,  0.06421615, -0.09981593, -0.04676081],\n",
       "       [ 0.00582715, -0.00082042,  0.08949279, -0.05860555, -0.06086115],\n",
       "       [-0.0073965 ,  0.01508697, -0.06315924,  0.03610879,  0.07375108],\n",
       "       [ 0.02676571, -0.06018451,  0.04901674, -0.06820899, -0.09190667],\n",
       "       [ 0.06546219, -0.01183401,  0.06592977, -0.09727801, -0.02433181],\n",
       "       [-0.00600783,  0.05120589, -0.01553274,  0.05495261,  0.00937006]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
